# Import necessary libraries
import re
import numpy as np
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
from tensorflow.keras.callbacks import EarlyStopping
from transformers import logging
from datasets import load_dataset

nltk.download('stopwords')
nltk.download('punkt')

# Step 1: we need to load the dataset
dataset = load_dataset("McAuley-Lab/Amazon-Reviews-2023", "raw_review_All_Beauty", trust_remote_code=True)
data = pd.DataFrame(dataset["full"])

# for this we are using smaller dataset for faster approach
data = data[['rating', 'text']].dropna().sample(10000, random_state=42)

# Step 2: we are using clean text for our code
def clean_text(text):
    text = re.sub('<.*?>', '', text)  
    text = re.sub('[^a-zA-Z]', ' ', text)  
    text = text.lower()
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)

data['clean_text'] = data['text'].astype(str).apply(clean_text)

# Step 3: we are creating lables that help us for finding
def label_sentiment(rating):
    if rating >= 4:
        return 2  # Positive
    elif rating <= 2:
        return 0  # Negative
    else:
        return 1  # Neutral

data['sentiment'] = data['rating'].apply(label_sentiment)

# Step 4: now we are providing token and padding
MAX_WORDS = 10000
MAX_LEN = 100

tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token='<OOV>')
tokenizer.fit_on_texts(data['clean_text'])
sequences = tokenizer.texts_to_sequences(data['clean_text'])
padded = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')

X = np.array(padded)
y = np.array(data['sentiment'])

# Step 5: Train-Validation-Test Split
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Step 6: we have build an lstm model
model = Sequential([
    Embedding(MAX_WORDS, 100, input_length=MAX_LEN),
    LSTM(128, return_sequences=True),
    Dropout(0.3),
    LSTM(64),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense(3, activation='softmax')
])

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Step 7: last part is training
early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=5,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)

# Step 8: and then we need to evluate
y_pred = np.argmax(model.predict(X_test), axis=1)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred, average='weighted'))
print("Recall:", recall_score(y_test, y_pred, average='weighted'))
print("F1 Score:", f1_score(y_test, y_pred, average='weighted'))
print("ROC-AUC:", roc_auc_score(pd.get_dummies(y_test), pd.get_dummies(y_pred), multi_class='ovr'))

print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=['Negative','Neutral','Positive']))

# Step 9: finally the predictions
sample_indices = np.random.choice(len(X_test), 5)
for i in sample_indices:
    print(f"\nReview: {data.iloc[i]['text'][:200]}")
    print(f"Actual: {['Negative','Neutral','Positive'][y_test[i]]}, Predicted: {['Negative','Neutral','Positive'][y_pred[i]]}")
